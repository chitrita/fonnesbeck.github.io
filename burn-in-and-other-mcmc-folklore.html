
<!DOCTYPE HTML>
<!--
	Dopetrope 2.0 by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
			<title>Strong Inference</title>
			<meta http-equiv="content-type" content="text/html; charset=utf-8" />
			<meta charset="utf-8" />
			<link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,900,300italic" rel="stylesheet" />
				<link rel="stylesheet" href="/theme/css/pygment.css" />
			<noscript>
				<link rel="stylesheet" href="/theme/css/skel-noscript.css" />
				<link rel="stylesheet" href="/theme/css/style.css" />
				<link rel="stylesheet" href="/theme/css/style-desktop.css" />
			</noscript>
	</head>
	<body class="no-sidebar">
		<!-- Header Wrapper -->
			<div id="header-wrapper">
				<div class="container">
					<div class="row">
						<div class="12u">
						
							<!-- Header -->
								<section id="header">
									
									<!-- Logo -->
									<h1><a href="http://stronginference.com">Strong Inference</a></h1>
									
									<!-- Nav -->
										<nav id="nav">
											<ul>
											</ul>
										</nav>

								</section>

						</div>
					</div>
				</div>
			</div>
		
		<!-- Main Wrapper -->
			<div id="main-wrapper">
				<div class="container">
<div class="row">
	<div class="12u">
		<!-- Portfolio -->
			<section>
				<div>
					<div class="row">
						<div class="12u skel-cell-mainContent">
							<!-- Content -->
								<article class="box is-post">
									<a href="#" class="image image-full"><img src=""/></a>
									<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>I have been slowly working my way through <a href="http://amzn.to/mR9PVr">The Handbook of Markov Chain Monte Carlo</a>, a compiled volume edited by Steve Brooks <em>et al.</em> that I picked up at last week's Joint Statistical Meetings. The first chapter is a primer on MCMC by <a href="http://www.stat.umn.edu/~charlie/">Charles Geyer</a>, in which he summarizes the key concepts of the theory and application of MCMC. In a particularly provocative passage, Geyer rips several of the traditional practices in setting up, running and diagnosing MCMC runs, including multi-chain runs, burn-in and sample-based diagnostics. Though they are applied regularly, these steps are simply heuristics that are applied to either aid in reaching or identifying the equilibrium distribution of the Markov chain. There are no guarantees on the reliability of any of them.</p>
<p>In particular, he questions the utility of burn-in:</p>
<blockquote>
<p>Burn-in is only one method, and not a particuarly good method, for finding a good starting point.</p>
</blockquote>
<p>I can't disagree with this, though I have always viewed MCMC sampling (for most models that I have dealt with) as being cheap enough that there is little cost to simply throwing away thousands of them. I have often thrown away as many as the first 90 percent of my samples! However, as Geyer notes, there are better ways of getting your chain into a decent region of its support without throwing anything away.</p>
<p>One method is to use an approximation method on your model before applying MCMC. For example, the <a href="http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">maximum a posteriori (MAP)</a> estimate can be obtained using numerical optimization, then used as the initial values for an MCMC run. It turns out to be pretty easy to do in PyMC. For example, using the built-in bioassay example:</p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="kn">from</span> <span class="nn">pymc.examples</span> <span class="kn">import</span> <span class="n">gelman_bioassay</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="kn">from</span> <span class="nn">pymc</span> <span class="kn">import</span> <span class="n">MAP</span><span class="p">,</span> <span class="n">MCMC</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="n">M</span> <span class="o">=</span> <span class="n">MAP</span><span class="p">(</span><span class="n">gelman_bioassay</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">M</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>


<p>This yields MAP estimates for all the parameters in the model, which are less likely to be true modes as the complexity of the model increases, but are a pretty good bet to be a decent starting point for MCMC.</p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">M</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">value</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">array</span><span class="p">(</span><span class="mf">0.8465802225061101</span><span class="p">)</span>
</pre></div>


<p>All that remains is to move these estimates into an MCMC sampler. While one could manually plug the values of each node into the model specification, its easiest just to extract the variables from the MAP estimator, and use them to instantiate an <code>MCMC</code> object:</p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">M</span><span class="o">.</span><span class="n">variables</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> 
<span class="nb">set</span><span class="p">([</span><span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">PyMCObjects</span><span class="o">.</span><span class="n">Stochastic</span> <span class="s">&#39;alpha&#39;</span> <span class="n">at</span> <span class="mh">0x10f78e810</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">PyMCObjects</span><span class="o">.</span><span class="n">Stochastic</span> <span class="s">&#39;beta&#39;</span> <span class="n">at</span> <span class="mh">0x10f78e910</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">PyMCObjects</span><span class="o">.</span><span class="n">Deterministic</span> <span class="s">&#39;theta&#39;</span> <span class="n">at</span> <span class="mh">0x10f78e9d0</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Binomial</span> <span class="s">&#39;deaths&#39;</span> <span class="n">at</span> <span class="mh">0x10f78ea50</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">CommonDeterministics</span><span class="o">.</span><span class="n">Lambda</span> <span class="s">&#39;LD50&#39;</span> <span class="n">at</span> <span class="mh">0x10f78ec10</span><span class="o">&gt;</span><span class="p">])</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="n">MC</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">MC</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">Sampling</span><span class="p">:</span> <span class="mi">100</span><span class="o">%</span> <span class="p">[</span><span class="mo">0000000000000000000000000000000000000000000000</span><span class="p">]</span> <span class="n">Iterations</span><span class="p">:</span> <span class="mi">1000</span>
</pre></div>


<p>Notice that I did not pass a <code>burn</code> argument to MCMC, which defaults to zero. As is evident from the graphical output of the posteriors, this results in what appears to be a homogeneous chain, and which is hopefully already at its equilibrium distribution.</p>
<p><img src="http://f.cl.ly/items/4513263v3x3n1T0m3o27/alpha.png" width="500"></p>
<p><img src="http://f.cl.ly/items/1i0W0k1Q2S3h172E2v0b/beta.png" width="500"></p>
<p>What the MCMC practitioner fears is using a chain for inference that has not yet converged to its target distribution. Unfortunately, diagnostics cannot reliably alert you to this, nor does starting a model in several chains from disparate starting values guarantee this. There is also no magical threshold to distinguish convergence from pre-convergence regions in a MCMC trace. Geyer insists that only running chains for a very, very long time will inspire confidence:</p>
<blockquote>
<p>Your humble author has a dictum that the lease one can do is make an overnight run. ... If you do not make runs like that, you are simply not serious about MCMC.</p>
</blockquote>
								</article>
						</div>
					</div>
				</div>
			</section>
	</div>
</div>

				</div>
			</div>

		<!-- Footer Wrapper -->
			<div id="footer-wrapper">
				<!-- Footer -->
					<section id="footer" class="container">
						<div class="row">
							<div class="8u">
								<section>
									<header>
										<h2>Latest articles</h2>
									</header>
									<ul class="dates">
										<li>
											<span class="date">Nov <strong>30</strong></span>
											<h3><a href="bayes-factors-pymc.html">Calculating Bayes factors with PyMC</a></h3>
											<p><script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>Statisticians are sometimes interested in comparing two (or more) models, with respect to their relative support by a particular dataset. This may be in order to select the best model to use for inference, or ...</p></p>
										</li>
										<li>
											<span class="date">Aug <strong>09</strong></span>
											<h3><a href="burn-in-and-other-mcmc-folklore.html">Burn-in, and Other MCMC Folklore</a></h3>
											<p><script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>I have been slowly working my way through <a href="http://amzn.to/mR9PVr">The Handbook of Markov Chain Monte Carlo</a>, a compiled volume edited by Steve Brooks <em>et al.</em> that I picked up at last week's Joint Statistical Meetings ...</p></p>
										</li>
										<li>
											<span class="date">Mar <strong>07</strong></span>
											<h3><a href="implementing-dirichlet-processes-for-bayesian-semi-parametric-models.html">Implementing Dirichlet processes for Bayesian semi-parametric models</a></h3>
											<p><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>Semi-parametric methods have been preferred for a long time in survival analysis, for example, where the baseline hazard function is expressed non-parametrically to avoid assumptions regarding its form. Meanwhile, the ...</p></p>
										</li>
										<li>
											<span class="date">Aug <strong>18</strong></span>
											<h3><a href="missing-data-imputation.html">Automatic Missing Data Imputation with PyMC</a></h3>
											<p><script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>A distinct advantage of using Bayesian inference is in its universal application of probability models for providing inference. As such, all components of a Bayesian model are specified using probability distributions for either describing a ...</p></p>
										</li>
									</ul>
								</section>
							</div>
						</div>
						<div class="row">
							<div class="4u">
								<section>
									<header>
										<h2>Blogroll</h2>
									</header>
									<ul class="divided">
											<li><a href="http://andrewgelman.com/">Statistical Modeling, Causal Inference, and Social Science</a></li>
											<li><a href="http://www.johndcook.com/blog/">John D. Cook</a></li>
											<li><a href="https://jakevdp.github.io">Pythonic Perambulations</a></li>
											<li><a href="http://healthyalgorithms.com">Healthy Algorithms</a></li>
									</ul>
								</section>
							</div>
							<div class="4u">
								<section>
									<header>
										<h2>Categories</h2>
									</header>
									<ul class="divided">
											<li><a href="http://stronginference.com/category/statistics.html">Statistics</a></li>
									</ul>
								</section>
							</div>
							<div class="4u">
							
								<section>
									<header>
										<h2>Contact</h2>
									</header>
									<ul class="social">
									</ul>
								</section>
							</div>
						</div>
					</section>
			</div>
		<script src="/theme/js/jquery.min.js"></script>
		<script src="/theme/js/jquery.dropotron.js"></script>
		<script src="/theme/js/config.js"></script>
		<script src="/theme/js/skel.min.js"></script>
		<script src="/theme/js/skel-panels.min.js"></script>
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><link rel="stylesheet" href="/theme/css/ie8.css" /><![endif]-->
	</body>
</html>